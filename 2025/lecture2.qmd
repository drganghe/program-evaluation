---
title: "Lecture 2 Goals and Types of Evaluation"
description-meta: "Why do evaluation? Major forms evaluation."
author: "Gang He"
date: Feb 4, 2025
date-format: long
image: "/images/high-line-park.jpg"
format:
  revealjs:
    theme: white
 #   embed-resources: true
    slide-number: true
    preview-links: auto
    link-external-newwindow: true
    citations-hover: true
    auto-stretch: true
    r-fit-text: true
    center: true
    scrollable: true
    css: /custom.css
    footer: <https://drganghe.github.io/program-evaluation/>
#draft: true
---

## Recap lecture 1

- Get to know each other
- John Snow and evidence based analysis
- Evidence based policy
  - Global clean energy supply chain
  - Rand Health Insurance Experiment
- Capturing the wedage
- Not all policies are based on evidence

## Goal

Sensible program evaluation at reasonable cost

1.  Funder
2.  Government/NGO/Public Institutions/Citizens
3.  Program manager/staff
4.  Executives
5.  **Evidence-based policy and management decisions**


## Program & Program Evaluation{.smaller}

::: columns

::: {.column width="50%"}
Program:  

>A program is a set of resources and activities directed toward one or more common goals, typically under the direction of a single manager or management team.
:::

::: {.column width="50%"}
Program Evaluation: 

> Program evaluation is the application of systematic methods to address questions about program operations and results. It may include ongoing monitoring of a program as well as one-shot studies of program processes or program impact. The approaches used are based on social science research methodologies and professional standards.
:::

:::

::: footer
Source: @newcomer_handbook_2015
:::


## Five questions to ask

- Can the results of the evaluation influence decisions about the program?
- Can the evaluation be done in time to be useful?
- Is the program significant enough to merit evaluation?
- Is program performance viewed as problematic?
- Where is the program in its development?

::: footer
Source: @newcomer_handbook_2015
:::

## Type of evaluation

<center>
Formative &harr; Summative   
Ongoing &harr; One-Shot   
Objective &harr; Observers Participatory  
Goal-Oriented &harr; “Goal-Free”   
Quantitative &harr; Qualitative   
Ex Ante &harr; Post Program   
Problem Orientation &harr; Non-Problem  
</center>

::: footer
Source: @newcomer_handbook_2015
:::


## Types and uses of evaluation

::: {style="font-size: 16px;"}
|Evaluation Types|When to use|What it shows|Why it is useful|
|---|---|---|---|
|Formative Evaluation<br /> Evaluability Assessment<br /> Needs Assessment |- During the development of a new program. <br /> - When an existing program being modified or is being used in a new setting or with a new population. |- Whether the proposed program elements are likely to be needed, understood, and accepted by the population you want to reach. <br />- The extent to which an evaluation is possible, based on the goals and objectives. |- It allows for modification to be made to the plan before full implementation begins.<br />- Maximizes the likelihood that the program will succeed |
|Process Evaluation<br />Program Monitoring |- As soon as program implementation begins.<br />- During operation of an existing program.  | - How well the program is working<br />- The extent to which the program is being implemented as designed<br />- Whether the program is accessible and acceptable to its target population | - Provides an early warning for any problems that may occur<br />- Allows programs to monitor how well their program plans and activities are working |
|Outcome Evaluation<br /> Objectives-Based Evaluation | - After the program has made contact with at least one person or group in the target population  | - The degree to which the program is having an effect on the target population's behaviors. | - Tells whether the program is being effective in meeting its objectives. |
|Economic Evaluation: Cost Analysis, Cost-Effectiveness Evaluation, Cost-Benefit Analysis, Cost-Utility Analysis| - At the beginning of a program<br />- During the operation of an existing program. | - What resources are being used in a program and their costs (direct and indirect) compared to outcome | - Provides program managers and funders a way to assess cost relative to effects. "How much bang for your buck. |
|Impact Evaluation| - During the operation of an existing program at appropriate intervals. <br />- At the end of a program. | - The degree to which the program meets its ultimate goal on an overall rate of STD transmission (how much has program X decreased the morbidity of an STD beyond the study population).  | - Provides evidence for use in policy and funding decisions |
:::

::: footer
Source: [CDC](https://www.cdc.gov/std/program/pupestd/types%20of%20evaluation.pdf)
:::

<!---
## Process evaluation in the RIPPLE study

- The RIPPLE (randomised intervention of pupil peer led sex education) study: whether peer delivered sex education is more effective than teacher delivered sessions at decreasing risky sexual behaviour 
- It involves 27 English secondary schools and follow-up to age 19.
- The outcome results by age 16 showed that the peer led approach improved some knowledge outcomes; increased satisfaction with sex education; and reduced intercourse and increased confidence about the use of condoms in girls

::: footer
Source: @oakley_process_2006
:::
--->

## Process evaluation can be very useful

![](/images/lessons-learned-from-the-process-evaluations.png){fig-align="center"}

::: footer
Source: @limbani_process_2019
:::


## Kirkpatrick Model for training programs

![](/images/kirkpatrick-model.png){fig-align="center"}

::: footer    
Source: [Kirkpatrick Model](https://www.kirkpatrickpartners.com/the-kirkpatrick-model/)
:::


## UK MRC Framework

![](/images/uk-mrc-framework.jpg){fig-align="center"}

::: footer  
Source: @skivingtonNewFrameworkDeveloping2021
:::


## PRISM and RE-AIM

![](/images/prism-reaim-model.png){fig-align="center"}

::: footer
Source: @feldsteinPracticalRobustImplementation2008
:::



## Exploratory evaluation

::: {style="font-size: 20px;"}
| Approach | Purpose | Time |
|---|---|---|
| Evaluability assessment | Assess whether programs are ready for useful evaluation; get agreement on program goals and evaluation criteria; clarify the focus and intended use of further evaluation. | 1 to 6 months; 2 staff-weeks to 3 staff-months|
| Rapid feedback evaluation | Estimate program effectiveness in terms of agreed-on program goals; indicate the range of uncertainty in the estimates; produce tested designs for more definitive evaluation; clarify the focus and intended use of further evaluation. | 3 to 6 months; 3 to 12 staff-months |
| Evaluation synthesis | Synthesize findings of prior research and evaluation studies. | 1 to 4 months; 1 to 3 staff-months |
| Small-sample studies | Estimate program effectiveness in terms of agreed-on program goals; produce tested measures of program performance. | 1 week to 6 months; 1 staff-week to 12 staff-month |
:::

::: footer
Source: @newcomer_handbook_2015
:::

## Evaluability Standards: Readiness for Useful Evaluation

- Program goals are agreed on and realistic.
- Information needs are well defined.
- Evaluation data are obtainable.
- Intended users are willing and able to use evaluation information.

::: footer
Source: @newcomer_handbook_2015
:::

## Selecting an exploratory evaluation approach

::: {style="font-size: 24px;"}
| Approach | When Appropriate|
|---|---|
| Evaluability assessment | Large, decentralized program; unclear evaluation criteria.|
| Rapid feedback evaluation | Agreement exists on the goals in terms of which a program is to be evaluated; need for evaluation information “right quick”; potential need for more definitive evaluation. |
| Evaluation synthesis | Need for evaluation information “right quick”; potential need for more definitive evaluation. |
| Small-sample studies | Agreement exists on the goals in terms of which a program is to be evaluated; collection of evaluation data will require sampling.|
:::

::: footer
Source: @newcomer_handbook_2015
:::


## High Line Park Story

[![](/images/high-line-park.jpg){.r-stretch fig-align="center"}](/stories/high-line-park.qmd)

::: footer
Photo credit: Gang He; High Line Observation Deck@[10th Ave & W 17th St](https://maps.app.goo.gl/vcQFaFYRndg8F5Kt9)
:::


## Culturally responsive evaluation

- Culture of the program that is being evaluated
- Cultural parameters 
- Including equity 


## CRE framework

![](/images/cultural-responsive-evaluation-framework.png){fig-align="center"}

::: footer
Source: @newcomer_handbook_2015
:::


## Evaluation process and feedback

![](/images/figure1.3-proces-feedback.png){fig-align="center"}

::: footer
Source: @newcomer_handbook_2015
:::


## GAO’s Evaluation Design Process

- Clarify the study objectives; 
- Obtain background information on the issue and design options;
- Develop and test the proposed approach; 
- Reach agreement on the proposed approach.

::: footer
Read more: [Designing Evaluations: 2012 Revision](https://www.gao.gov/assets/gao-12-208g.pdf)
:::



------------------------------------------------------------------------

### References

::: {#refs}
:::

::: footer
Back to Title [Slide](lecture2.qmd), All [Lectures](schedule.qmd), Course [Home](https://drganghe.github.io/program-evaluation/), or Learn More About [Gang He](https://drganghe.github.io)
:::
